<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>NICE: Non-linear Independent Components Estimation | fastpages</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="NICE: Non-linear Independent Components Estimation" />
<meta name="author" content="[Arxiv](https://arxiv.org/abs/1410.8516)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="L. Dinh, D. Krueger, and Y. Bengio" />
<meta property="og:description" content="L. Dinh, D. Krueger, and Y. Bengio" />
<link rel="canonical" href="https://talwarabhimanyu.github.io/paper-notes-v1/representation-learning/2020/06/21/non-linear-independent-components-estimation.html" />
<meta property="og:url" content="https://talwarabhimanyu.github.io/paper-notes-v1/representation-learning/2020/06/21/non-linear-independent-components-estimation.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-21T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://talwarabhimanyu.github.io/paper-notes-v1/representation-learning/2020/06/21/non-linear-independent-components-estimation.html","@type":"BlogPosting","headline":"NICE: Non-linear Independent Components Estimation","dateModified":"2020-06-21T00:00:00-05:00","datePublished":"2020-06-21T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://talwarabhimanyu.github.io/paper-notes-v1/representation-learning/2020/06/21/non-linear-independent-components-estimation.html"},"author":{"@type":"Person","name":"[Arxiv](https://arxiv.org/abs/1410.8516)"},"description":"L. Dinh, D. Krueger, and Y. Bengio","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/paper-notes-v1/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://talwarabhimanyu.github.io/paper-notes-v1/feed.xml" title="fastpages" /><link rel="shortcut icon" type="image/x-icon" href="/paper-notes-v1/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/paper-notes-v1/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/paper-notes-v1/about/">About Me</a><a class="page-link" href="/paper-notes-v1/search/">Search</a><a class="page-link" href="/paper-notes-v1/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">NICE: Non-linear Independent Components Estimation</h1><p class="page-description">L. Dinh, D. Krueger, and Y. Bengio</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-06-21T00:00:00-05:00" itemprop="datePublished">
        Jun 21, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">[Arxiv](https://arxiv.org/abs/1410.8516)</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      3 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/paper-notes-v1/categories/#representation-learning">representation-learning</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Key-Ideas">Key Ideas </a></li>
<li class="toc-entry toc-h2"><a href="#Design-of-a-Good-Transformation">Design of a Good Transformation </a></li>
<li class="toc-entry toc-h2"><a href="#Architecture-of-the-Neural-Network-used-for-the-Transformation">Architecture of the Neural Network used for the Transformation </a>
<ul>
<li class="toc-entry toc-h3"><a href="#General-Coupling-Layer">General Coupling Layer </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Appendix-1">Appendix 1 </a></li>
<li class="toc-entry toc-h2"><a href="#References">References </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-06-21-non-linear-independent-components-estimation.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Key-Ideas">
<a class="anchor" href="#Key-Ideas" aria-hidden="true"><span class="octicon octicon-link"></span></a>Key Ideas<a class="anchor-link" href="#Key-Ideas"> </a>
</h2>
<p>We want to learn probability distributions which generate real world data. But sometimes, instead of dealing with raw data, dealing with some representation of it makes it easier to learn its generative distribution. E.g. this representation may get rid of all the noise, and retain enough information to be able to explain the variance in this real world data.</p>
<p>We want to find a "good representation" ($h \in \mathbb{R}^m$) of data ($x \in \mathbb{R}^n$) by applying transformations ($h = f(x)$). The authors define a good representation as "one in which the distribution of the data is easy to model" and in this paper they propose a transformation such that the distribution of transformed data completely factorizes (i.e. $p_H(h) = \prod_dp_{H_d}(h_d)$).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Design-of-a-Good-Transformation">
<a class="anchor" href="#Design-of-a-Good-Transformation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Design of a Good Transformation<a class="anchor-link" href="#Design-of-a-Good-Transformation"> </a>
</h2>
<p>After applying the transformation $f$ (assume it is invertible, and assume $m=n$), the distribution of the original random variable can be written as (see Appendix 1 for proof):
$$
p_X(x) = p_H\left(f(x)\right)det(J) 
$$</p>
<p>Now say we observe $N$ samples of $X$, then we estimate the transformation $f$ (a parametrized neural network) by maximizing the log-likelihood of these samples (while assuming some prior distribution $p_H$). The log-likelihood can be written as:</p>
$$
\log{p_X(x)} = \sum_{d=1}^D\log{p_{H_d}(f_d(x))} + \log{\lvert det\left(\frac{\partial f(x)}{\partial x}\right)\rvert}
$$<p>The authors observe that a good transformation should be able to capture a complex distribution and at the same time is should be easy to compute:</p>
<ul>
<li>Its inverse (so that we can easily sample from $p_X$ in two steps: (1) $h \sim p_H$, (2) $x = f^{-1}(h)$).</li>
<li>The determinant of its Jacobian matrix (because that appears in the log-likelihood expression we are trying to maximize).</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Architecture-of-the-Neural-Network-used-for-the-Transformation">
<a class="anchor" href="#Architecture-of-the-Neural-Network-used-for-the-Transformation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Architecture of the Neural Network used for the Transformation<a class="anchor-link" href="#Architecture-of-the-Neural-Network-used-for-the-Transformation"> </a>
</h2>
<p>Let's first see what makes for an easily computable Jacobian determinant. Now the determinant of a triangular matrix is just the product of its diagonal elements <sup id="fnref-3" class="footnote-ref"><a href="#fn-3">3</a></sup>. With this in mind, the authors introduce a family of invertible functions with triangular Jacobians, called "Couplings".</p>
<h3 id="General-Coupling-Layer">
<a class="anchor" href="#General-Coupling-Layer" aria-hidden="true"><span class="octicon octicon-link"></span></a>General Coupling Layer<a class="anchor-link" href="#General-Coupling-Layer"> </a>
</h3>
<p>If $x \in \mathbb{R}^D$, then we define $y \in \mathbb{R}^D$ such that the first $d$ components of $y$ are the same as that of $x$, and the other $D-d$ components are determined by a function $g: \mathbb{R}^{D-d} \times m\left(\mathbb{R}^{d}\right) \to \mathbb{R}^{D-d}$ called the "Coupling Law":</p>
$$
y_{1\cdots d} = x_{1\cdots d} \\
y_{d+1\cdots D} = g\left(x_{1\cdots d}, m\left(x_{d+1\cdots D}\right)\right)
$$<p>We see (from the way we defined this transformation above) that the Jacobian for this transformation of $x$ into $y$ is triangular:</p>
$$
\frac{\partial y}{\partial x} = \begin{bmatrix}
I_d &amp; 0 \\
\frac{\partial y_{d+1\cdots D}}{\partial x_{1\cdots d}} &amp; \frac{\partial y_{d+1\cdots D}}{\partial x_{d+1\cdots D}}
\end{bmatrix}
$$
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Appendix-1">
<a class="anchor" href="#Appendix-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Appendix 1<a class="anchor-link" href="#Appendix-1"> </a>
</h2>
<p><code>A proof for Equation (1) from the paper.</code></p>
<p>Here is a proof for 2D based on notes from Prof. Ash <sup id="fnref-1" class="footnote-ref"><a href="#fn-1">1</a></sup> and Prof. Dobelman <sup id="fnref-2" class="footnote-ref"><a href="#fn-2">2</a></sup>:</p>
<p>Let $f: \mathbb{R}^2 \to \mathbb{R}^2$ be the transformation. Let $h = (h_1, h_2)$ and $x = (x_1, x_2)$ be realizations of $H$ and $X$ respectively, such that we have $f(h) = x$. Say $x_1$ changes by $dx_1$, then the change in $h_1$ and $h_2$ is $(\partial h_1/\partial x_1)dx_1$ and $(\partial h_2/\partial x_1)dx_1$ respectively. Similarly, if $x_2$ changes by $dx_2$, then the change in $h_1$ and $h_2$ is $(\partial h_1/\partial x_2)dx_2$ and $(\partial h_2/\partial x_2)dx_2$ respectively. Now consider the small rectangle with lengths $dx_1$ and $dx_2$ in the $x_1-x_2$ plane. This corresponds to a parallelogram in the $h_1-h_2$ plane with sides:</p>
$$
\vec{A} = \begin{bmatrix}\frac{\partial h_1}{\partial x_1}dx_1 \\ \frac{\partial h_2}{\partial x_1}dx_1\end{bmatrix} \\
\vec{B} = \begin{bmatrix}\frac{\partial h_1}{\partial x_2}dx_2 \\ \frac{\partial h_2}{\partial x_2}dx_2\end{bmatrix}
$$<p>The area of this parallelogram is given by the magnitude of the cross product $\vec{A} \times \vec{B}$, and that equals $det(J)dx_1dx_2$, where $J$ is the Jacobian matrix, written as:
$$
J = \begin{bmatrix}
\frac{\partial h_1}{\partial x_1} &amp; \frac{\partial h_1}{\partial x_2} \\
\frac{\partial h_2}{\partial x_1} &amp; \frac{\partial h_2}{\partial x_2}
\end{bmatrix}
$$
Now the probability mass of the rectangle in $x_1-x_2$ plane is the same as that of the parallelogram in the $h_1-h_2$ plane. This is because the authors assume the transformation $f$ is invertible. So we can write:
$$
p_H(h)det(J)dx_1dx_2 = p_X(x)dx_1dx_2 \\
\implies p_X(x) = p_H(f(x))det(J) 
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="References">
<a class="anchor" href="#References" aria-hidden="true"><span class="octicon octicon-link"></span></a>References<a class="anchor-link" href="#References"> </a>
</h2>
<p></p>
<div class="footnotes"><p id="fn-1">1. <a href="https://faculty.math.illinois.edu/~r-ash/Stat/StatLec1-5.pdf">Transformation of Random Variables</a><a href="#fnref-1" class="footnote footnotes">↩</a></p></div>
<div class="footnotes"><p id="fn-2">2. <a href="http://www.stat.rice.edu/~dobelman/notes_papers/math/Jacobian.pdf">Jacobians</a><a href="#fnref-2" class="footnote footnotes">↩</a></p></div>
<div class="footnotes"><p id="fn-3">3. <a href="https://math.stackexchange.com/questions/2013124/determinant-of-a-triangular-matrix">StackExchange - Determinant of a Triangular Matrix</a><a href="#fnref-3" class="footnote footnotes">↩</a></p></div>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="talwarabhimanyu/paper-notes-v1"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/paper-notes-v1/representation-learning/2020/06/21/non-linear-independent-components-estimation.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/paper-notes-v1/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/paper-notes-v1/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/paper-notes-v1/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/paper-notes-v1/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/paper-notes-v1/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
