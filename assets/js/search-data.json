{
  
    
        "post0": {
            "title": "Random Graphs with Arbitrary Degree Distributions and their Applications",
            "content": "Key Ideas from the Paper . Certain real world networks (e.g. a community of people) could be abstracted by a Random Graph where vertex degree (no. of edges incident on the vertex) follows some distribution, e.g. flip a coin to decide if two nodes should be connected. Such abstractions could be used to study phenomenon such as passage of disease through a community. Studies of Random Graphs so far assumed vertex degree to have the Poisson distribution, but this paper studies graphs with arbitrary degree distributions. . Random Graphs with Poisson Distributed Vertex Degree . Studies so far assumed that presence of an edge between any two nodes is determined by a coin flip with probability of $1$ being $p$. Further, presence of an edge betwen two nodes is assumed independent of presence of other edges in the network. These assumptions imply that vertex degree has a Poisson distribution (as we will see). . The authors state: . If there are $N$ vertices in the graph, and each is connected to an average of $z$ edges, then it is trivial to show that $p=z/(N-1)$. . Here is my short proof for this statement. . Let $E$ be the set of edges. We can say:$$ begin{align} &amp; mathbb{E}[|E|] = frac{zN}{2} implies &amp; frac{1}{2} sum_{i=1}^Np(N-1) = frac{zN}{2} implies &amp; p = frac{z}{N-1} implies &amp; p sim frac{z}{N} tag{for large $N$ (Eq. 1)} end{align} $$ . Given the independence assumption, the distribution of degree of a vertex, $p_k$, is thus Binomial, which for large $N$ can be approximated by a Poisson distribution. . begin{align} p_k &amp;= {N choose k}p^k(1-p)^{N-k} sim frac{z^ke^{-z}}{k!} tag{Eq. 2} end{align} Introducing Generating Functions . The authors express the probability distribution of a discrete random variables (e.g. degree of a vertex) as what is called a Generating Function2. They then exploit properties of Generating Functions to derive some interesting insights, e.g. finding the distribution of sums of random variables. . A sequence of numbers can be encapsulated into a function (the Generating Function), with the property that when that function is expressed as the sum of a power series ($a_0 + a_1x + a_2x^2 + cdots$), the coefficients of this series ($a_0,a_1,a_2, cdots$) give us the original number sequence2. E.g. For a graph with Poisson distributed vertex degree, $p_k$ (the probability a vertex has degree $k$) is a sequence whose Generating Function can be written as: $$ begin{align} G_0(x) &amp;= sum_{k=0}^N{N choose k}p^k(1-p)^{N-k}x^k &amp;= (1-p+px)^N &amp; sim e^{z(x-1)} tag{for $N to infty$ (Eq. 3)} end{align} $$ Several properties of the Generating Function $G_0(x) = sum_{k=0}^Np_kx^k$ come handy. . Moments: The average degree can be written as: $$ begin{align} z &amp;= sum_kkp_k &amp;= G&#39;_0(1) tag{First Moment (Eq. 4)} end{align} $$ For the case of Poisson distributed vertex degree, we have $G&#39;_0(1) = z$. . | Powers: The authors note (with my [annotations]): . If the distribution of a property $k$ [such as vertex degree] of an object [vertex of a graph] is generated by a given Generating Function [$G_0(x)$], then the distribution of the total $k$ summed over $m$ independent realizations of the object [sum of degrees of $m$ randomly chosen vertices] is generated by the $m^{th}$ power of that Generating Function [$G_0(x)^m$]. . So the coefficient of $x^k$ in $G_0(x)^m$ is the probability that the sum of degrees of $m$ randomly chosen vertices is $k$. With this we can start talking about distribution of sums of degrees of vertices. . | . Degree of a Vertex arrived at by following a random Edge . Intuitively, the distribution of degree of such a vertex will be different from that of a randomly chosen vertex ($p_k$) because by following a random edge, we are more likely to end up on a vertex with a high degree than at one with a relatively lower degree. And so the probability distribution of degree of a vertex chosen as such, would be skewed towards higher degree values. Formally, let $k_i$ be the degree of vertex $V_i$, then the probability of arriving at $V_i$ will be $ frac{k_i}{ sum_{j=0}^Nk_j} propto k_i$. And so the probability that a vertex chosen by following a random edge has degree $k$ is proportional to the probability that this vertex is chosen (which is $ propto k$) $ times$ probability this vertex has degree $k$ (which is $p_k$). . And so the authors note: . ... the vertex therefore has a probability distribution of degree proportional to $kp_k$. . And so for coefficientes ($p&#39;_k propto kp_k$) of the Generating Function to sum to 1, we need to normalize:$ frac{ sum_kkp_kx^k}{ sum_kkp_k} = x frac{G&#39;_0(x)}{G&#39;_0(1)}$. . Distribution of Number of Neighbors $m$ Steps Away . Let&#39;s first look at neighbors 1 steps away. We choose a random vertex and follow its edges to its immediate neighbors, and consider the distribution of degree of one such neighbor. The authors indicate that this distribution will be the same as that when we arrive at this vertex by following a random edge. This wasn&#39;t clear to me at first because in this case we are randomly choosing a vertex not an edge. But then it struck that vertices with high degrees are connected to more vertices than those with a low degree, and so are more likely to be arrived at when we pick a vertex randomly and visit its neighbors. The author&#39;s note for neighbors 1 step away: . ... the vertices arrived at each have the distribution of remaining outgoing edges generated by this function, less one power of $x$, to allow for the edge that we arrived along. . $$ begin{align} G_1(x) &amp;= frac{G&#39;_0(x)}{G&#39;_0(1)} &amp;= frac{1}{z}G&#39;_0(x) tag{Eq. 5} end{align} $$Now let&#39;s consider the distribution of second-neighbors of a vertex $v$, i.e. the sum of number of neighbors of immediate neighbors of $v$. (Note that this sum excludes the number of immediate neighbors). Let&#39;s first assume it is given that $v$ has $k$ immediate neighbors. Then the probability that it has $j$ second-neighbors, $p^{second}_{j|k}$ is the coefficient of $x^j$ in the Generating Function $G_1(x)^k$ (from the &quot;Powers&quot; property of Generating Functions). So we can get $p^{second}_{j}$ by simply marginalizing over the number of immediate neighbors of $v$. And thust the authors note:&gt; ... the probability distribution of second neighbors of the original vertex can be written as: $$ sum_{k}p_k[G_1(x)]^k = G_0(G_1(x)) tag{Eq. 6} $$ . Distribution of Component Sizes . Armed with the background knowledge above, the authors discuss distributions of interesting properties of random graphs, such as the size of connected components. . The authors assert the following and I&#39;m unable to prove it thus far (but let&#39;s take it for granted for now): . ... the chances of a connected component containing a close loop of edges goes as $N^{-1}$ which is negligible in the limit of large $N$. . The authors denote by $H_1(x)$:&gt; ... the generating function for the distribution of the sizes of components which are reached by choosing a random edge and following it to one of its ends. Now if $q_k$ be the probability that the vertex $v$ we arrive at (by following a random edge) has $k$ other incident edges. Then the probability that this component (the one containing $v$) has size $j$, is given by: . $$ begin{align} p^ text{component size}_j &amp;= q_0 cdots tag{Eq. 7} &amp;+ q_1 times text{(coefficient of }x^{j-1} text{ in }H_1(x) cdots &amp;+ q_2 times text{(coefficient of }x^{j-1} text{ in }[H_1(x)]^2 cdots &amp;+ text{and so on} end{align} $$And thus the authors assert: . $H_1(x)$ must satisfy a self-consistency condition of the form . $$ begin{align} H_1(x) = xq_0 + xq_1H_1(x) + xq_2[H_1(x)]^2 + cdots tag{Eq. 8} end{align} $$We know from results given before (see Eq. 5) that $q_k$ is just the coefficient of $x^k$ in $G_1(x)$, and so the self-consistency condition can also be written as:$$ begin{align} H_1(x) = G_1(H_1(x)) tag{Eq. 9} end{align} $$ . Following a similar line of reasoning as I&#39;ve shared in Eq. 7, if $H_0(x)$ is the Generating Function for the distribution of size of the component which contains a randomly chosen vertex, then we can show: $$ begin{align} H_0(x) = G_0(H_1(x)) tag{Eq. 10} end{align} $$ . Mean Component Size . The authors note that in practice Eq. 9 is complicated and rarely has a closed form solution for $H_1$ (given $G_1$). That said, we can still compute average component size using Eqs. 9 &amp; 10. Using the Moments property of Generating Functions (see Eq. 4), let $ langle s rangle$ denote the average size of the component to which a randomly chosen vertex belongs, then the authors note: . $$ begin{align} langle s rangle &amp;= H&#39;_0(1) &amp;= G_0(H_1(1)) + G&#39;_0(H_1(1))H&#39;_1(1) &amp;= 1 + G&#39;_0(1)H&#39;_1(1) tag{Eq. 11} end{align} $$The last step follows from the fact that $G_0(1) = H_1(1) = 1$ (this is required to normalize the distributions representated by $G_0$ and $H_1$ - so that the coefficients in each of those series sum to 1). . From Eq. 9, the authors note: $$ begin{align} H&#39;_1(1) &amp;= 1 + G&#39;_1(1)H&#39;_1(1) tag{Eq. 12} end{align} $$ . And from Eqs. 11 &amp; 12, after eliminating $H&#39;_1(1)$, we get (here $z_1$ is the average degree of a vertex and $z_2$ is the average number of second neighbors): $$ begin{align} langle s rangle &amp;= 1 + frac{G&#39;_0(1)}{1 - G&#39;_1(1)} tag{Eq. 13} &amp;= 1 + frac{z_1^2}{z_1-z_2} tag{Eq. 14} end{align} $$ . Let&#39;s look at that how $ langle s rangle$ behavse versus $z_1$ and $z_2$. We can see from the plot below that for each value of $z_1$, as the value of $z_2$ starts to exceed $z_1$, the mean component size explodes. This phenomenon is called a &quot;Phase Transition&quot;, at which point a &quot;Giant Component&quot; (a component of size $ Omega(N)$ 3) appears in the graph. Phase Transitions (see Chapter 4, Foundations of Data Science by Profs. Blum, Hopcraft and Kannan 3) are characterized by abrupt transitions in a graph (such as appearance of cycles) once the probability $p$ (of there being an edge between two random vertices, crosses a certain threshold). . import warnings warnings.filterwarnings(&quot;ignore&quot;) import numpy as np import matplotlib.pyplot as plt from matplotlib import cm z1 = np.arange(2, 100) z2 = np.arange(2, 100) xx, yy = np.meshgrid(z1, z2) s = 1 + xx**2/(xx - yy) fig, ax = plt.subplots() for z1 in np.linspace(0.1,1.0, 10): z2 = np.linspace(0.1, z1,10) s = 1 + z1**2/(z1 - z2) _ = ax.plot(z2, s) ax.annotate(f&quot;z1={z1:.2f}&quot;, (z1*0.9, s[-2]*1.05)) ax.set_xlabel(&quot;z2 (Mean No. of Second Neighbors)&quot;) ax.set_ylabel(r&quot;$ langle s rangle$ (Mean Component Size)&quot;) ax.set_title(r&quot;Plot of $ langle s rangle$ versus $z_1$, $z_2$&quot;, fontsize=14) fig.set_size_inches(10,6) . References . 1. View on Arxiv↩ . 2. Herbert S. Wilf, generatingfunctionology. View PDF on author s website↩ . 3. Chapter 4, Foundations of Data Science, Avrim Blum, John Hopcroft, and Ravindran Kannan. PDF from authors website.↩ .",
            "url": "https://talwarabhimanyu.github.io/paper-notes-v1/graph-theory/random-graphs/2020/07/01/random-graphs-with-arbitrary-degree-dist.html",
            "relUrl": "/graph-theory/random-graphs/2020/07/01/random-graphs-with-arbitrary-degree-dist.html",
            "date": " • Jul 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "NICE: Non-linear Independent Components Estimation",
            "content": "Key Ideas . We want to learn probability distributions which generate real world data. But sometimes, instead of dealing with raw data, dealing with some representation of it makes it easier to learn its generative distribution. E.g. this representation may get rid of all the noise, and retain enough information to be able to explain the variance in this real world data. . We want to find a &quot;good representation&quot; ($h in mathbb{R}^m$) of data ($x in mathbb{R}^n$) by applying transformations ($h = f(x)$). The authors define a good representation as &quot;one in which the distribution of the data is easy to model&quot; and in this paper they propose a transformation such that the distribution of transformed data completely factorizes (i.e. $p_H(h) = prod_dp_{H_d}(h_d)$). . Design of a Good Transformation . After applying the transformation $f$ (assume it is invertible, and assume $m=n$), the distribution of the original random variable can be written as (see Appendix 1 for proof): $$ begin{align} p_X(x) &amp;= p_H left(f(x) right)det(J) end{align} $$ . Now say we observe $N$ samples of $X$, then we estimate the transformation $f$ (a parametrized neural network) by maximizing the log-likelihood of these samples (while assuming some prior distribution $p_H$). The log-likelihood can be written as: . $$ log{p_X(x)} = sum_{d=1}^D log{p_{H_d}(f_d(x))} + log{ lvert det left( frac{ partial f(x)}{ partial x} right) rvert} $$The authors observe that a good transformation should be able to capture a complex distribution and at the same time is should be easy to compute: . Its inverse (so that we can easily sample from $p_X$ in two steps: (1) $h sim p_H$, (2) $x = f^{-1}(h)$). | The determinant of its Jacobian matrix (because that appears in the log-likelihood expression we are trying to maximize). | . Architecture of the Neural Network used for the Transformation . Let&#39;s first see what makes for an easily computable Jacobian determinant. Now the determinant of a triangular matrix is just the product of its diagonal elements 3. With this in mind, the authors introduce a family of invertible functions with triangular Jacobians, called &quot;Couplings&quot;. . General Coupling Layer . If $x in mathbb{R}^D$, then we define $y in mathbb{R}^D$ such that the first $d$ components of $y$ are the same as that of $x$, and the other $D-d$ components are determined by a function $g: mathbb{R}^{D-d} times m left( mathbb{R}^{d} right) to mathbb{R}^{D-d}$ called the &quot;Coupling Law&quot;: . $$ begin{align} y_{1 cdots d} &amp;= x_{1 cdots d} y_{d+1 cdots D} &amp;= g left(x_{1 cdots d}, m left(x_{d+1 cdots D} right) right) end{align} $$We see (from the way we defined this transformation above) that the Jacobian for this transformation of $x$ into $y$ is triangular: . $$ begin{align} frac{ partial y}{ partial x} &amp;= begin{bmatrix} I_d &amp; 0 frac{ partial y_{d+1 cdots D}}{ partial x_{1 cdots d}} &amp; frac{ partial y_{d+1 cdots D}}{ partial x_{d+1 cdots D}} end{bmatrix} end{align} $$ Appendix 1 . A proof for Equation (1) from the paper. . Here is a proof for 2D based on notes from Prof. Ash 1 and Prof. Dobelman 2: . Let $f: mathbb{R}^2 to mathbb{R}^2$ be the transformation. Let $h = (h_1, h_2)$ and $x = (x_1, x_2)$ be realizations of $H$ and $X$ respectively, such that we have $f(h) = x$. Say $x_1$ changes by $dx_1$, then the change in $h_1$ and $h_2$ is $( partial h_1/ partial x_1)dx_1$ and $( partial h_2/ partial x_1)dx_1$ respectively. Similarly, if $x_2$ changes by $dx_2$, then the change in $h_1$ and $h_2$ is $( partial h_1/ partial x_2)dx_2$ and $( partial h_2/ partial x_2)dx_2$ respectively. Now consider the small rectangle with lengths $dx_1$ and $dx_2$ in the $x_1-x_2$ plane. This corresponds to a parallelogram in the $h_1-h_2$ plane with sides: . $$ begin{align} vec{A} &amp;= begin{bmatrix} frac{ partial h_1}{ partial x_1}dx_1 frac{ partial h_2}{ partial x_1}dx_1 end{bmatrix} vec{B} &amp;= begin{bmatrix} frac{ partial h_1}{ partial x_2}dx_2 frac{ partial h_2}{ partial x_2}dx_2 end{bmatrix} end{align} $$The area of this parallelogram is given by the magnitude of the cross product $ vec{A} times vec{B}$, and that equals $det(J)dx_1dx_2$, where $J$ is the Jacobian matrix, written as: $$ begin{align} J &amp;= begin{bmatrix} frac{ partial h_1}{ partial x_1} &amp; frac{ partial h_1}{ partial x_2} frac{ partial h_2}{ partial x_1} &amp; frac{ partial h_2}{ partial x_2} end{bmatrix} end{align} $$ Now the probability mass of the rectangle in $x_1-x_2$ plane is the same as that of the parallelogram in the $h_1-h_2$ plane. This is because the authors assume the transformation $f$ is invertible. So we can write: $$ begin{align} p_H(h)det(J)dx_1dx_2 &amp; = p_X(x)dx_1dx_2 implies p_X(x) &amp;= p_H(f(x))det(J) end{align} $$ . References . 1. Transformation of Random Variables↩ . 2. Jacobians↩ . 3. StackExchange - Determinant of a Triangular Matrix↩ .",
            "url": "https://talwarabhimanyu.github.io/paper-notes-v1/representation-learning/2020/06/21/non-linear-independent-components-estimation.html",
            "relUrl": "/representation-learning/2020/06/21/non-linear-independent-components-estimation.html",
            "date": " • Jun 21, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! I am Abhimanyu and you can find more about me here. I love reading papers and taking notes, and I created this blog to save my notes for future reference. I hope someone else may find them useful too. .",
          "url": "https://talwarabhimanyu.github.io/paper-notes-v1/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://talwarabhimanyu.github.io/paper-notes-v1/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}